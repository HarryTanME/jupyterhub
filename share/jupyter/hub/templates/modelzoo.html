{% extends "page.html" %}

{% block main %}
<link rel="stylesheet" href="http://wode.ai/Portals/_default/Containers/20073-UnlimitedColorsPack-055/container.css?cdv=104" type="text/css">
<link rel="stylesheet" href="http://wode.ai/Portals/_default/skins/20073-unlimitedcolorspack-055/skin.css?cdv=104" type="text/css">

<div class="container">
<div class="col-sm-9">
<dl class="dl-horizontal">


    <tr>
     <dt><strong>Name</strong></dt><dd><strong>caffe2_models</strong></dd>
     <dt><strong>Size</strong></dt><dd>12M</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/caffe2/models</dd>
     <dt><strong>Description</strong></dt><dd>This is a repository for storing pre-trained Caffe2 models. You can use Caffe2 to help you download or install the models on your machine.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>ConceptNet</strong></dd>
     <dt><strong>Size</strong></dt><dd>5.0G</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/commonsense/conceptnet-numberbatch</dd>
     <dt><strong>Description</strong></dt><dd>"ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning.<p/>ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings.<p/>It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting. It is described in the paper ConceptNet 5.5: An Open Multilingual Graph of General Knowledge, presented at AAAI 2017.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>GloVe</strong></dd>
     <dt><strong>Size</strong></dt><dd>5.0G</dd>
     <dt><strong>Source</strong></dt><dd>https://nlp.stanford.edu/projects/glove/</dd>
     <dt><strong>Description</strong></dt><dd>"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>GoogleNews</strong></dd>
     <dt><strong>Size</strong></dt><dd>3.4G</dd>
     <dt><strong>Source</strong></dt><dd>https://code.google.com/archive/p/word2vec/</dd>
     <dt><strong>Description</strong></dt><dd>This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>ResNet</strong></dd>
     <dt><strong>Size</strong></dt><dd>99M</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/KaimingHe/deep-residual-networks</dd>
     <dt><strong>Description</strong></dt><dd>"Deep Residual Net, the original models (ResNet-50, ResNet-101, and ResNet-152) described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). These models are those used in [ILSVRC] (http://image-net.org/challenges/LSVRC/2015/) and COCO 2015 competitions, which won the 1st places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>spaCy</strong></dd>
     <dt><strong>Size</strong></dt><dd>2.4G</dd>
     <dt><strong>Source</strong></dt><dd>https://spacy.io/usage/models</dd>
     <dt><strong>Description</strong></dt><dd>"All the language models that are pre-trained in spaCy. spaCy v2.0 features new neural models for tagging, parsing and entity recognition. The models have been designed and implemented from scratch specifically for spaCy, to give you an unmatched balance of speed, size and accuracy. A novel bloom embedding strategy with subword features is used to support huge vocabularies in tiny tables. Convolutional layers with residual connections, layer normalization and maxout non-linearity are used, giving much better efficiency than the standard BiLSTM solution. For more details, see the notes on the model architecture.<p/>The parser and NER use an imitation learning objective to deliver accuracy in-line with the latest research systems, even when evaluated from raw text. With these innovations, spaCy v2.0's models are 10Ã— smaller, 20% more accurate, and even cheaper to run than the previous generation.</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>squeezenet</strong></dd>
     <dt><strong>Size</strong></dt><dd>9.6M</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/DeepScale/SqueezeNet</dd>
     <dt><strong>Description</strong></dt><dd>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size</dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>vgg16</strong></dd>
     <dt><strong>Size</strong></dt><dd>528M</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/fchollet/deep-learning-models/releases</dd>
     <dt><strong>Description</strong></dt><dd></dd></tr>
    <tr>
     <dt><strong>Name</strong></dt><dd><strong>vgg19</strong></dd>
     <dt><strong>Size</strong></dt><dd>550M</dd>
     <dt><strong>Source</strong></dt><dd>https://github.com/fchollet/deep-learning-models/releases</dd>
     <dt><strong>Description</strong></dt><dd></dd>
    </tr>


    </dl>
</div>
</div>


{% endblock %}

{% block script %}
{{ super() }}
    <script type="text/javascript">
       
    </script>
{% endblock %}